{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10651756,"sourceType":"datasetVersion","datasetId":6592409},{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899,"modelId":1902},{"sourceId":250822,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":214407,"modelId":236079}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:51:31.797144Z","iopub.execute_input":"2025-02-05T18:51:31.797433Z","iopub.status.idle":"2025-02-05T18:51:38.854508Z","shell.execute_reply.started":"2025-02-05T18:51:31.797402Z","shell.execute_reply":"2025-02-05T18:51:38.853406Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.27.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: torch~=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.10)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch~=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch~=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate, bitsandbytes\nSuccessfully installed bitsandbytes-0.45.1 evaluate-0.4.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\n***** Finetuning using a Trainer class from the Huggingface Transformers\n***** library.\n\"\"\"\n# Imports\nimport numpy as np\nimport torch\nimport evaluate\nimport transformers\nfrom transformers import AutoTokenizer, TrainingArguments, Seq2SeqTrainingArguments, Trainer, Seq2SeqTrainer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:51:38.855650Z","iopub.execute_input":"2025-02-05T18:51:38.855973Z","iopub.status.idle":"2025-02-05T18:52:01.240925Z","shell.execute_reply.started":"2025-02-05T18:51:38.855940Z","shell.execute_reply":"2025-02-05T18:52:01.240229Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Global variables\nFILENAME = \"/kaggle/input/jane-austens-works/ja1-train.json\"\nVALID_FILE = \"/kaggle/input/jane-austens-works/ja1-valid.json\"\nDS = {\"train\":FILENAME, \"valid\":VALID_FILE}\nMODEL = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\nDEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nOUTPUT_DIR = \"/kaggle/working/\"\nML = 80\nBS = 64\nCHECK_DIR = \"/kaggle/working/checkpoint-250\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:52:01.241613Z","iopub.execute_input":"2025-02-05T18:52:01.242139Z","iopub.status.idle":"2025-02-05T18:52:01.315846Z","shell.execute_reply.started":"2025-02-05T18:52:01.242117Z","shell.execute_reply":"2025-02-05T18:52:01.314692Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"dataset = load_dataset(\"json\", data_files=DS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:52:01.317766Z","iopub.execute_input":"2025-02-05T18:52:01.318021Z","iopub.status.idle":"2025-02-05T18:52:01.812394Z","shell.execute_reply.started":"2025-02-05T18:52:01.318000Z","shell.execute_reply":"2025-02-05T18:52:01.811571Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3c099f2f3df4237a3a6c2903ec5a28d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating valid split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a18da2143ec4fd1aa93d41c7e372dcb"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Initializing tokenizer and preprocessing input, etc.\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# Bugfix for padding issues: https://discuss.huggingface.co/t/mistral-trouble-when-fine-tuning-dont-set-pad-token-id-eos-token-id/77928/8\ntokenizer.add_special_tokens({'pad_token': '<pad>'})\n\ndef tokenize_function(examples): return tokenizer(examples[\"text\"], \n                                                  padding=\"max_length\", \n                                                  truncation=True,\n                                                  max_length = ML,\n                                                  return_tensors=\"pt\")\nfull = dataset.map(tokenize_function, batched=True, batch_size=BS)\n\nencoded_input = full[\"train\"]\nencoded_valid = full[\"valid\"]\n\nprint(\"Preprocessed and tokenized data.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:52:01.813746Z","iopub.execute_input":"2025-02-05T18:52:01.813980Z","iopub.status.idle":"2025-02-05T18:52:09.236962Z","shell.execute_reply.started":"2025-02-05T18:52:01.813958Z","shell.execute_reply":"2025-02-05T18:52:09.235930Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/66769 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0350b3bed5b245ef8626f97cd6f44d07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13169 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c48b96cb3c8844e5965a3468b2abe94f"}},"metadata":{}},{"name":"stdout","text":"Preprocessed and tokenized data.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"lengths = {}\nfor i in range(10000):\n    t = len(encoded_input[i]['input_ids'])\n    if t in lengths:\n        lengths[t] += 1\n    else:\n        lengths[t] = 1\nprint(lengths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:52:09.237852Z","iopub.execute_input":"2025-02-05T18:52:09.238127Z","iopub.status.idle":"2025-02-05T18:52:10.412857Z","shell.execute_reply.started":"2025-02-05T18:52:09.238092Z","shell.execute_reply":"2025-02-05T18:52:10.412032Z"}},"outputs":[{"name":"stdout","text":"{80: 10000}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Preparing for training and setting eval function\ntargs = Seq2SeqTrainingArguments(output_dir = OUTPUT_DIR,\n                                 learning_rate = 2e-5,\n                                 warmup_steps=2,\n                                 gradient_accumulation_steps = 1,\n                                 logging_dir=OUTPUT_DIR+\"logs/\",\n                                 logging_steps = 10,\n                                 per_device_train_batch_size=BS,\n                                 #per_device_eval_batch_size=4,\n                                 bf16=True,\n                                 optim=\"paged_adamw_8bit\",\n                                 save_strategy = \"steps\",\n                                 save_steps = 250,\n                                 #eval_strategy = \"steps\",\n                                 #eval_steps = 250,\n                                 do_eval = False,\n                                 report_to = \"none\",\n                                 num_train_epochs = 1,\n                                 use_cpu = False,\n                                 log_level=\"debug\",\n                                 save_total_limit = 2,\n                                )\nmetric = evaluate.load(\"accuracy\")\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\nprint(\"Checkpoint.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:52:10.413684Z","iopub.execute_input":"2025-02-05T18:52:10.413979Z","iopub.status.idle":"2025-02-05T18:52:11.302417Z","shell.execute_reply.started":"2025-02-05T18:52:10.413956Z","shell.execute_reply":"2025-02-05T18:52:11.301784Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"070dd460b2f44c63acfd63d267326e42"}},"metadata":{}},{"name":"stdout","text":"Checkpoint.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Retrieving pretrained model\nqc = BitsAndBytesConfig(load_in_4bit=True,\n                        bnb_4bit_quant_type=\"nf4\",\n                        bnb_4bit_compute_dtype=torch.float16,\n                       )\nmodel = AutoModelForCausalLM.from_pretrained(MODEL, quantization_config = qc, device_map=\"auto\")\n\n# Bugfix for padding issues: https://discuss.huggingface.co/t/mistral-trouble-when-fine-tuning-dont-set-pad-token-id-eos-token-id/77928/8\nmodel.resize_token_embeddings(len(tokenizer))\n\nprint(\"Retrieved model.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:52:11.303205Z","iopub.execute_input":"2025-02-05T18:52:11.303538Z","iopub.status.idle":"2025-02-05T18:53:57.766385Z","shell.execute_reply.started":"2025-02-05T18:52:11.303502Z","shell.execute_reply":"2025-02-05T18:53:57.765605Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc178b620bc54333b5581b26275eacf0"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\nThe new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"name":"stdout","text":"Retrieved model.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n\nconfig = LoraConfig(\n    r=32,\n    lora_alpha=64,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:53:57.767316Z","iopub.execute_input":"2025-02-05T18:53:57.767627Z","iopub.status.idle":"2025-02-05T18:53:58.852269Z","shell.execute_reply.started":"2025-02-05T18:53:57.767602Z","shell.execute_reply":"2025-02-05T18:53:58.851342Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Trainer object\ntrainer = Seq2SeqTrainer(\n    model = model,\n    args = targs,\n    train_dataset = encoded_input,\n    eval_dataset = encoded_valid,\n    compute_metrics = compute_metrics,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)\n\nprint(\"Created Trainer object.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:53:58.853112Z","iopub.execute_input":"2025-02-05T18:53:58.853347Z","iopub.status.idle":"2025-02-05T18:53:58.872060Z","shell.execute_reply.started":"2025-02-05T18:53:58.853327Z","shell.execute_reply":"2025-02-05T18:53:58.871395Z"}},"outputs":[{"name":"stderr","text":"Using auto half precision backend\n","output_type":"stream"},{"name":"stdout","text":"Created Trainer object.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nmodel.config.use_cache = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:53:58.872659Z","iopub.execute_input":"2025-02-05T18:53:58.872876Z","iopub.status.idle":"2025-02-05T18:53:58.876691Z","shell.execute_reply.started":"2025-02-05T18:53:58.872856Z","shell.execute_reply":"2025-02-05T18:53:58.875693Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!rm -rf /kaggle/working/*\n!cp -r \"/kaggle/input/checkpoint-750/transformers/default/1\" \"/kaggle/working/checkpoint-750/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:56:19.076238Z","iopub.execute_input":"2025-02-05T18:56:19.076757Z","iopub.status.idle":"2025-02-05T18:56:20.516823Z","shell.execute_reply.started":"2025-02-05T18:56:19.076716Z","shell.execute_reply":"2025-02-05T18:56:20.515663Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"trainer.train(resume_from_checkpoint = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:56:40.489542Z","iopub.execute_input":"2025-02-05T18:56:40.489876Z","iopub.status.idle":"2025-02-05T22:20:13.059229Z","shell.execute_reply.started":"2025-02-05T18:56:40.489853Z","shell.execute_reply":"2025-02-05T22:20:13.058540Z"}},"outputs":[{"name":"stderr","text":"Loading model from /kaggle/working/checkpoint-750.\nCurrently training with a batch size of: 64\nThe following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 66,769\n  Num Epochs = 1\n  Instantaneous batch size per device = 64\n  Total train batch size (w. parallel, distributed & accumulation) = 64\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1,044\n  Number of trainable parameters = 85,041,184\n  Continuing training from checkpoint, will skip to saved global_step\n  Continuing training from epoch 0\n  Continuing training from global step 750\n  Will skip the first 0 epochs then the first 750 batches in the first epoch.\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3081: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint_rng_state = torch.load(rng_file)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1044' max='1044' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1044/1044 3:22:49, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>760</td>\n      <td>2.404400</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>2.403500</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>2.411000</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>2.389300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.422000</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>2.410400</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>2.363300</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>2.344300</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>2.381500</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>2.425500</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>2.345300</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>2.387000</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>2.365500</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>2.408600</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.426000</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>2.401200</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>2.345700</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>2.377300</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>2.365100</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>2.373200</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>2.385300</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>2.401100</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>2.389600</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>2.387400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.373700</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>2.379900</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>2.376900</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>2.372000</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>2.425100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to /kaggle/working/checkpoint-1000\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nSaving model checkpoint to /kaggle/working/checkpoint-1044\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\nDeleting older checkpoint [/kaggle/working/checkpoint-750] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1044, training_loss=0.6724419703428772, metrics={'train_runtime': 12211.1463, 'train_samples_per_second': 5.468, 'train_steps_per_second': 0.085, 'total_flos': 2.3337163993402368e+17, 'train_loss': 0.6724419703428772, 'epoch': 1.0})"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"checkpoint-1044\", 'zip', \"/kaggle/working/checkpoint-1044\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:27:46.706057Z","iopub.execute_input":"2025-02-05T22:27:46.706394Z","iopub.status.idle":"2025-02-05T22:31:50.178794Z","shell.execute_reply.started":"2025-02-05T22:27:46.706367Z","shell.execute_reply":"2025-02-05T22:31:50.177961Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/checkpoint-1044.zip'"},"metadata":{}}],"execution_count":16}]}